# æœºå™¨å­¦ä¹ çš„çœŸå®å·¥ä½œ - è¿œä¸æ­¢è°ƒå‚ï¼

## ğŸ¯ å¸¸è§è¯¯è§£

```
è¯¯è§£ï¼šæœºå™¨å­¦ä¹  = è°ƒç”¨åº“ + è°ƒå‚
               âŒ å¤ªç®€åŒ–äº†ï¼

çœŸç›¸ï¼šæœºå™¨å­¦ä¹  = 
      æ•°æ®å¤„ç†(60%) + 
      ç‰¹å¾å·¥ç¨‹(20%) + 
      æ¨¡å‹é€‰æ‹©å’Œè°ƒå‚(10%) + 
      è¯„ä¼°å’Œä¼˜åŒ–(10%)
```

---

## ğŸ“Š çœŸå®é¡¹ç›®çš„æ—¶é—´åˆ†é…

```
ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼ˆå‡è®¾10å¤©ï¼‰ï¼š

ğŸ“¦ æ•°æ®æ”¶é›†å’Œæ¸…æ´—ï¼š      4-5å¤©  (40-50%) ğŸ”¥ æœ€è€—æ—¶ï¼
ğŸ”§ ç‰¹å¾å·¥ç¨‹ï¼š            2-3å¤©  (20-30%)
ğŸ¤– æ¨¡å‹è®­ç»ƒå’Œè°ƒå‚ï¼š      1-2å¤©  (10-20%) â† ä½ ä»¥ä¸ºçš„"å…¨éƒ¨"
ğŸ“ˆ è¯„ä¼°å’Œä¼˜åŒ–ï¼š          1å¤©    (10%)
ğŸš€ éƒ¨ç½²å’Œç»´æŠ¤ï¼š          æŒç»­...

ç»“è®ºï¼šè°ƒå‚åªå å¾ˆå°ä¸€éƒ¨åˆ†ï¼
```

---

## ğŸ” è¯¦ç»†æ‹†è§£ï¼šæœºå™¨å­¦ä¹ çš„æ¯ä¸ªæ­¥éª¤

### ç¬¬1æ­¥ï¼šé—®é¢˜å®šä¹‰ï¼ˆ1å°æ—¶ - 1å¤©ï¼‰

**ä¸æ˜¯è°ƒç”¨åº“ï¼Œè€Œæ˜¯æ€è€ƒ**ï¼š

```python
# é”™è¯¯çš„å¼€å§‹
model = SVC()  # æˆ‘è¦ç”¨SVMï¼

# æ­£ç¡®çš„å¼€å§‹
â“ é—®é¢˜æ¸…å•ï¼š
   1. è¿™æ˜¯åˆ†ç±»è¿˜æ˜¯å›å½’é—®é¢˜ï¼Ÿ
   2. éœ€è¦å®æ—¶é¢„æµ‹å—ï¼Ÿï¼ˆå½±å“ç®—æ³•é€‰æ‹©ï¼‰
   3. å¯è§£é‡Šæ€§é‡è¦å—ï¼Ÿï¼ˆKNNå¯è§£é‡Šï¼Œç¥ç»ç½‘ç»œä¸è¡Œï¼‰
   4. æ•°æ®é‡æœ‰å¤šå¤§ï¼Ÿï¼ˆå°æ•°æ®ç”¨SVMï¼Œå¤§æ•°æ®ç”¨æ·±åº¦å­¦ä¹ ï¼‰
   5. å‡†ç¡®ç‡è¦æ±‚å¤šé«˜ï¼Ÿï¼ˆ95%è¿˜æ˜¯99%ï¼Ÿï¼‰
   6. è®¡ç®—èµ„æºé™åˆ¶ï¼Ÿï¼ˆæœ‰GPUå—ï¼Ÿï¼‰
```

**å®é™…æ¡ˆä¾‹**ï¼š

```
é¡¹ç›®ï¼šé¢„æµ‹ç”¨æˆ·æ˜¯å¦ä¼šæµå¤±

âŒ ç›´æ¥ä¸Šæ¥å°±ï¼šmodel = RandomForestClassifier()

âœ… å…ˆæ€è€ƒï¼š
   - ä¸šåŠ¡ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå‡å°‘æµå¤±è¿˜æ˜¯æé«˜æ”¶å…¥ï¼Ÿï¼‰
   - å‡é˜³æ€§å’Œå‡é˜´æ€§å“ªä¸ªä»£ä»·æ›´å¤§ï¼Ÿ
   - éœ€è¦å®æ—¶é¢„æµ‹è¿˜æ˜¯æ‰¹é‡é¢„æµ‹ï¼Ÿ
   - æ¨¡å‹éœ€è¦å¯è§£é‡Šå—ï¼Ÿï¼ˆå‘é«˜å±‚æ±‡æŠ¥ä¸ºä»€ä¹ˆé¢„æµ‹æŸç”¨æˆ·ä¼šæµå¤±ï¼‰
```

---

### ç¬¬2æ­¥ï¼šæ•°æ®æ”¶é›†ï¼ˆå‡ å°æ—¶ - å‡ å‘¨ï¼‰

**çœŸå®æŒ‘æˆ˜**ï¼š

```python
# ç†æƒ³çŠ¶æ€ï¼ˆæ•™å­¦æ•°æ®é›†ï¼‰
from sklearn import datasets
X, y = datasets.load_digits()  # å®Œç¾çš„æ•°æ®ï¼âœ¨

# ç°å®çŠ¶æ€ï¼ˆçœŸå®é¡¹ç›®ï¼‰
# æ•°æ®æ•£è½åœ¨ï¼š
- MySQLæ•°æ®åº“ï¼ˆç”¨æˆ·ä¿¡æ¯ï¼‰
- MongoDBï¼ˆæ—¥å¿—æ•°æ®ï¼‰
- S3å­˜å‚¨æ¡¶ï¼ˆå›¾ç‰‡ï¼‰
- ç¬¬ä¸‰æ–¹APIï¼ˆå¤©æ°”æ•°æ®ï¼‰
- Excelè¡¨æ ¼ï¼ˆä¸šåŠ¡éƒ¨é—¨æä¾›ï¼‰ğŸ˜±
- è¿˜æœ‰äº›æ•°æ®éœ€è¦çˆ¬è™«è·å–...

# ä½ éœ€è¦åšï¼š
import pandas as pd
import pymysql
import boto3
from bs4 import BeautifulSoup

# å†™ä¸€å †ä»£ç æ•´åˆæ•°æ®
df1 = pd.read_sql("SELECT ...", connection)
df2 = pd.read_csv("s3://bucket/data.csv")
df3 = scrape_website("https://...")
...

# å‡ å¤©åç»ˆäºæœ‰äº†æ•°æ® ğŸ˜“
```

---

### ç¬¬3æ­¥ï¼šæ•°æ®æ¢ç´¢å’Œæ¸…æ´—ï¼ˆâ­ æœ€é‡è¦ï¼40-50%çš„æ—¶é—´ï¼‰

è¿™ä¸€æ­¥**æ²¡æœ‰ç°æˆçš„åº“å¯ä»¥ä¸€é”®å®Œæˆ**ï¼

#### 3.1 æ¢ç´¢æ•°æ®

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# åŠ è½½æ•°æ®
df = pd.read_csv('user_data.csv')

# å¼€å§‹æ¢ç´¢ï¼ˆè¿™æ˜¯ç»éªŒå’ŒæŠ€æœ¯çš„ç»“åˆï¼‰
print(df.info())          # æœ‰å¤šå°‘è¡Œï¼Ÿä»€ä¹ˆç±»å‹ï¼Ÿ
print(df.describe())       # æ•°å€¼èŒƒå›´ï¼Ÿ
print(df.isnull().sum())   # ç¼ºå¤±å€¼ï¼Ÿ
print(df.duplicated().sum()) # é‡å¤å€¼ï¼Ÿ

# å¯è§†åŒ–åˆ†æ
df['age'].hist()          # å¹´é¾„åˆ†å¸ƒåˆç†å—ï¼Ÿ
df.boxplot(column='income') # æœ‰å¼‚å¸¸å€¼å—ï¼Ÿ
sns.heatmap(df.corr())    # ç‰¹å¾ç›¸å…³æ€§ï¼Ÿ

# å‘ç°é—®é¢˜ï¼š
# âŒ å¹´é¾„æœ‰-5å²çš„ï¼ˆé”™è¯¯æ•°æ®ï¼‰
# âŒ æ”¶å…¥æœ‰999999999ï¼ˆå¼‚å¸¸å€¼ï¼‰
# âŒ 30%çš„è®°å½•ç¼ºå¤±ç”µè¯å·ç 
# âŒ æ€§åˆ«å­—æ®µæœ‰ï¼šç”·/å¥³/M/F/1/0/male/femaleï¼ˆä¸ç»Ÿä¸€ï¼‰
```

#### 3.2 æ•°æ®æ¸…æ´—ï¼ˆçº¯æ‰‹å·¥ï¼ï¼‰

```python
# å¤„ç†ç¼ºå¤±å€¼ï¼ˆéœ€è¦é¢†åŸŸçŸ¥è¯†å†³å®šæ€ä¹ˆå¤„ç†ï¼‰
df['age'].fillna(df['age'].median(), inplace=True)  # ç”¨ä¸­ä½æ•°å¡«å……
df['email'].fillna('unknown@example.com', inplace=True)
df.dropna(subset=['user_id'], inplace=True)  # åˆ é™¤å…³é”®å­—æ®µç¼ºå¤±çš„è¡Œ

# å¤„ç†å¼‚å¸¸å€¼ï¼ˆéœ€è¦åˆ¤æ–­ï¼‰
df = df[df['age'] > 0]  # å¹´é¾„å¿…é¡»å¤§äº0
df = df[df['age'] < 120]  # å¹´é¾„åº”è¯¥å°äº120
df = df[df['income'] < 10000000]  # å¼‚å¸¸é«˜æ”¶å…¥

# å¤„ç†é‡å¤å€¼
df.drop_duplicates(subset=['user_id'], keep='first', inplace=True)

# æ ‡å‡†åŒ–æ ¼å¼
df['gender'] = df['gender'].map({
    'ç”·': 'M', 'male': 'M', '1': 'M',
    'å¥³': 'F', 'female': 'F', '0': 'F'
})

# å¤„ç†æ—¥æœŸ
df['signup_date'] = pd.to_datetime(df['signup_date'])

# è¿™å¯èƒ½èŠ±è´¹å‡ å¤©æ—¶é—´ï¼ğŸ˜°
```

#### 3.3 å¤„ç†ç±»åˆ«ä¸å¹³è¡¡

```python
# å‘ç°é—®é¢˜ï¼š
print(df['label'].value_counts())
# æ­£å¸¸ç”¨æˆ·: 9500  (95%)
# æµå¤±ç”¨æˆ·:  500  (5%)  â† æåº¦ä¸å¹³è¡¡ï¼

# éœ€è¦å¤„ç†ï¼ˆå¤šç§ç­–ç•¥ï¼‰ï¼š
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# ç­–ç•¥1: è¿‡é‡‡æ ·
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# ç­–ç•¥2: æ¬ é‡‡æ ·
undersampler = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = undersampler.fit_resample(X, y)

# ç­–ç•¥3: è°ƒæ•´ç±»åˆ«æƒé‡
model = SVC(class_weight='balanced')  # å‘Šè¯‰æ¨¡å‹ç±»åˆ«ä¸å¹³è¡¡

# è¿™éœ€è¦ç»éªŒå’Œå®éªŒï¼
```

---

### ç¬¬4æ­¥ï¼šç‰¹å¾å·¥ç¨‹ï¼ˆâ­ 20-30%çš„æ—¶é—´ï¼Œå†³å®šæ¨¡å‹ä¸Šé™ï¼‰

**è¿™æ˜¯åŒºåˆ†åˆçº§å’Œé«˜çº§å·¥ç¨‹å¸ˆçš„å…³é”®ï¼**

#### 4.1 ç‰¹å¾åˆ›å»º

```python
# åŸå§‹ç‰¹å¾
df['signup_date'] = '2023-01-15'
df['last_login'] = '2024-01-01'

# åˆ›é€ æ–°ç‰¹å¾ï¼ˆéœ€è¦é¢†åŸŸçŸ¥è¯†å’Œåˆ›é€ åŠ›ï¼ï¼‰
df['account_age_days'] = (df['last_login'] - df['signup_date']).dt.days
df['is_weekend_user'] = df['last_login'].dt.dayofweek >= 5
df['login_frequency'] = df['total_logins'] / df['account_age_days']
df['avg_session_duration'] = df['total_session_time'] / df['total_sessions']
df['is_premium'] = df['subscription_type'].isin(['premium', 'gold'])

# äº¤å‰ç‰¹å¾
df['age_income_ratio'] = df['age'] / (df['income'] + 1)
df['engagement_score'] = df['login_frequency'] * df['avg_session_duration']

# è¿™éœ€è¦ä¸šåŠ¡ç†è§£ï¼ä¸æ˜¯è°ƒç”¨åº“å°±èƒ½åšçš„
```

#### 4.2 ç‰¹å¾é€‰æ‹©

```python
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier

# æ–¹æ³•1: ç»Ÿè®¡æ£€éªŒ
selector = SelectKBest(f_classif, k=20)
X_selected = selector.fit_transform(X, y)

# æ–¹æ³•2: åŸºäºæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
rf = RandomForestClassifier()
rf.fit(X, y)
importances = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print(importances.head(10))  # çœ‹çœ‹å“ªäº›ç‰¹å¾æœ€é‡è¦

# å†³å®šä¿ç•™å“ªäº›ç‰¹å¾éœ€è¦å®éªŒå’Œåˆ¤æ–­ï¼
```

#### 4.3 ç‰¹å¾å˜æ¢

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder

# æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric)

# ç±»åˆ«ç‰¹å¾ç¼–ç 
le = LabelEncoder()
df['city_encoded'] = le.fit_transform(df['city'])

# One-hotç¼–ç 
df = pd.get_dummies(df, columns=['category', 'gender'])

# å¯¹æ•°å˜æ¢ï¼ˆå¤„ç†åæ–œåˆ†å¸ƒï¼‰
df['income_log'] = np.log1p(df['income'])

# è¿™äº›éƒ½éœ€è¦æ ¹æ®æ•°æ®ç‰¹ç‚¹å†³å®šï¼
```

---

### ç¬¬5æ­¥ï¼šæ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒï¼ˆâ† ä½ ä»¥ä¸ºçš„"å…¨éƒ¨å·¥ä½œ"ï¼‰

#### 5.1 é€‰æ‹©åˆé€‚çš„ç®—æ³•

**ä¸æ˜¯"æˆ‘å–œæ¬¢ç”¨SVM"ï¼Œè€Œæ˜¯ç³»ç»Ÿæ€§åœ°é€‰æ‹©**ï¼š

```python
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier

# åˆ›å»ºå€™é€‰æ¨¡å‹åˆ—è¡¨
models = {
    'Logistic Regression': LogisticRegression(),
    'SVM': SVC(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'Neural Network': MLPClassifier()
}

# å¿«é€Ÿå¯¹æ¯”ï¼ˆBaselineï¼‰
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"{name}: {score:.4f}")

# è¾“å‡ºï¼š
# Logistic Regression: 0.8234
# SVM: 0.8456
# Random Forest: 0.8678  â† çœ‹èµ·æ¥æœ€å¥½
# Gradient Boosting: 0.8723  â† æ›´å¥½ï¼
# Neural Network: 0.8512

# é€‰æ‹©ï¼šGradient Boosting å’Œ Random Forest è¿›ä¸€æ­¥ä¼˜åŒ–
```

#### 5.2 è°ƒå‚ï¼ˆè¿™æ‰æ˜¯ä½ è¯´çš„éƒ¨åˆ†ï¼‰

**ä½†è°ƒå‚ä¹Ÿä¸æ˜¯éšä¾¿è°ƒï¼**

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# æ–¹æ³•1: ç½‘æ ¼æœç´¢ï¼ˆç©·ä¸¾æ‰€æœ‰ç»„åˆï¼‰
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    RandomForestClassifier(),
    param_grid,
    cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
    scoring='f1',  # é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡
    n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰CPU
    verbose=2
)

grid_search.fit(X_train, y_train)
print("æœ€ä½³å‚æ•°:", grid_search.best_params_)
print("æœ€ä½³å¾—åˆ†:", grid_search.best_score_)

# æ–¹æ³•2: éšæœºæœç´¢ï¼ˆå¤§å‚æ•°ç©ºé—´æ—¶æ›´å¿«ï¼‰
from scipy.stats import randint, uniform

param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(5, 50),
    'min_samples_split': randint(2, 20),
    'learning_rate': uniform(0.01, 0.3)
}

random_search = RandomizedSearchCV(
    GradientBoostingClassifier(),
    param_distributions,
    n_iter=100,  # å°è¯•100ç§ç»„åˆ
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=2
)

random_search.fit(X_train, y_train)

# è¿™å¯èƒ½è¿è¡Œå‡ å°æ—¶åˆ°å‡ å¤©ï¼
```

**è°ƒå‚çš„æŠ€å·§**ï¼ˆä¸æ˜¯çè¯•ï¼‰ï¼š

```python
# 1. å…ˆè°ƒå…³é”®å‚æ•°
# æ ‘çš„æ•°é‡ï¼ˆn_estimatorsï¼‰
# å­¦ä¹ ç‡ï¼ˆlearning_rateï¼‰
# æ ‘çš„æ·±åº¦ï¼ˆmax_depthï¼‰

# 2. å†è°ƒæ¬¡è¦å‚æ•°
# min_samples_split, min_samples_leaf

# 3. ä½¿ç”¨å­¦ä¹ æ›²çº¿åˆ¤æ–­
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# ç”»å›¾åˆ†æï¼š
# è®­ç»ƒåˆ†æ•°é«˜ã€éªŒè¯åˆ†æ•°ä½ â†’ è¿‡æ‹Ÿåˆ â†’ å‡å°‘å¤æ‚åº¦
# è®­ç»ƒå’ŒéªŒè¯åˆ†æ•°éƒ½ä½ â†’ æ¬ æ‹Ÿåˆ â†’ å¢åŠ å¤æ‚åº¦
```

---

### ç¬¬6æ­¥ï¼šæ¨¡å‹è¯„ä¼°ï¼ˆä¸åªæ˜¯å‡†ç¡®ç‡ï¼ï¼‰

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)

# 1. åŸºç¡€æŒ‡æ ‡
y_pred = model.predict(X_test)
print("å‡†ç¡®ç‡:", accuracy_score(y_test, y_pred))
print("ç²¾ç¡®ç‡:", precision_score(y_test, y_pred))
print("å¬å›ç‡:", recall_score(y_test, y_pred))
print("F1åˆ†æ•°:", f1_score(y_test, y_pred))

# 2. æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
print("æ··æ·†çŸ©é˜µ:\n", cm)

# 3. è¯¦ç»†æŠ¥å‘Š
print(classification_report(y_test, y_pred))

# 4. ROCæ›²çº¿å’ŒAUC
y_prob = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)
print("AUC:", auc)

# 5. ä¸šåŠ¡æŒ‡æ ‡ï¼ˆæœ€é‡è¦ï¼ï¼‰
# å‡è®¾æŠ“ä½ä¸€ä¸ªæµå¤±ç”¨æˆ·èƒ½æŒ½å›1000å…ƒ
# ä½†è¯¯åˆ¤ä¸€ä¸ªæ­£å¸¸ç”¨æˆ·ä¼šæŸå¤±100å…ƒå®¢æˆ·ä½“éªŒ

cost_matrix = np.array([
    [0, -100],    # é¢„æµ‹æ­£å¸¸ï¼šæ­£ç¡®0å…ƒï¼Œé”™è¯¯-100å…ƒ
    [-1000, 0]    # é¢„æµ‹æµå¤±ï¼šé”™è¯¯-1000å…ƒï¼Œæ­£ç¡®0å…ƒ
])

# è®¡ç®—ä¸šåŠ¡æˆæœ¬
business_cost = (cm * cost_matrix).sum()
print("ä¸šåŠ¡æˆæœ¬:", business_cost)

# é€‰æ‹©æœ€ä¼˜é˜ˆå€¼ï¼ˆä¸ä¸€å®šæ˜¯0.5ï¼ï¼‰
optimal_threshold = find_optimal_threshold(y_test, y_prob, cost_matrix)
```

---

### ç¬¬7æ­¥ï¼šæ¨¡å‹è§£é‡Šï¼ˆé‡è¦ä½†å¸¸è¢«å¿½ç•¥ï¼‰

```python
import shap
import lime

# 1. SHAPå€¼ï¼ˆè§£é‡Šæ¯ä¸ªé¢„æµ‹ï¼‰
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# å¯è§†åŒ–ï¼šä¸ºä»€ä¹ˆé¢„æµ‹è¿™ä¸ªç”¨æˆ·ä¼šæµå¤±ï¼Ÿ
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])

# 2. LIMEï¼ˆå±€éƒ¨è§£é‡Šï¼‰
from lime.lime_tabular import LimeTabularExplainer

explainer = LimeTabularExplainer(
    X_train.values,
    feature_names=X_train.columns,
    class_names=['æ­£å¸¸', 'æµå¤±']
)

# è§£é‡Šå•ä¸ªé¢„æµ‹
explanation = explainer.explain_instance(
    X_test.iloc[0].values,
    model.predict_proba
)
explanation.show_in_notebook()

# è¿™å¯¹äºå‘ä¸šåŠ¡æ–¹æ±‡æŠ¥è‡³å…³é‡è¦ï¼
```

---

### ç¬¬8æ­¥ï¼šéƒ¨ç½²ï¼ˆå·¥ç¨‹æŒ‘æˆ˜ï¼‰

```python
# 1. ä¿å­˜æ¨¡å‹
import joblib
joblib.dump(model, 'model.pkl')
joblib.dump(scaler, 'scaler.pkl')

# 2. åˆ›å»ºé¢„æµ‹API
from flask import Flask, request, jsonify

app = Flask(__name__)

# åŠ è½½æ¨¡å‹
model = joblib.load('model.pkl')
scaler = joblib.load('scaler.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    # ç‰¹å¾å·¥ç¨‹ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
    features = preprocess(data)
    # æ ‡å‡†åŒ–
    features_scaled = scaler.transform(features)
    # é¢„æµ‹
    prediction = model.predict(features_scaled)
    probability = model.predict_proba(features_scaled)
    
    return jsonify({
        'prediction': int(prediction[0]),
        'probability': float(probability[0][1])
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)

# 3. DockeråŒ–
# Dockerfile
"""
FROM python:3.9
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]
"""

# 4. ç›‘æ§
# - é¢„æµ‹å»¶è¿Ÿ
# - æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼ˆæ•°æ®æ¼‚ç§»ï¼‰
# - é”™è¯¯ç‡
# - æœåŠ¡å¯ç”¨æ€§
```

---

### ç¬¬9æ­¥ï¼šæŒç»­ç»´æŠ¤å’Œä¼˜åŒ–

```python
# 1. A/Bæµ‹è¯•
# æ–°æ¨¡å‹ vs æ—§æ¨¡å‹
# å“ªä¸ªåœ¨å®é™…ä¸šåŠ¡ä¸­è¡¨ç°æ›´å¥½ï¼Ÿ

# 2. ç›‘æ§æ•°æ®æ¼‚ç§»
from scipy.stats import ks_2samp

# æ¯”è¾ƒè®­ç»ƒæ•°æ®å’Œçº¿ä¸Šæ•°æ®çš„åˆ†å¸ƒ
for col in X_train.columns:
    stat, p_value = ks_2samp(
        X_train[col],
        X_production[col]
    )
    if p_value < 0.05:
        print(f"è­¦å‘Š: {col} ç‰¹å¾åˆ†å¸ƒå‘ç”Ÿå˜åŒ–ï¼")

# 3. å®šæœŸé‡è®­ç»ƒ
# æ¯å‘¨/æ¯æœˆç”¨æ–°æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹

# 4. æ€§èƒ½è¿½è¸ª
# è®°å½•æ¯æ¬¡é¢„æµ‹çš„ç»“æœ
# å‡ å‘¨åè·å¾—çœŸå®æ ‡ç­¾
# è®¡ç®—å®é™…å‡†ç¡®ç‡
# å¦‚æœä¸‹é™ â†’ è§¦å‘é‡è®­ç»ƒ
```

---

## ğŸ¯ å®Œæ•´å¯¹æ¯”ï¼šä½ ä»¥ä¸ºçš„ vs å®é™…çš„

### ä½ ä»¥ä¸ºçš„æœºå™¨å­¦ä¹ å·¥ä½œ

```python
# 5è¡Œä»£ç æå®š
from sklearn.svm import SVC
model = SVC(C=10, gamma=0.001)  # è°ƒè°ƒå‚
model.fit(X, y)
print("å‡†ç¡®ç‡:", model.score(X_test, y_test))
```

### å®é™…çš„æœºå™¨å­¦ä¹ é¡¹ç›®

```python
# 1. æ•°æ®æ”¶é›†ï¼ˆå‡ å¤©ï¼‰
df1 = load_from_database()
df2 = load_from_api()
df3 = load_from_files()
df = merge_all_data(df1, df2, df3)

# 2. æ•°æ®æ¸…æ´—ï¼ˆå‡ å¤©ï¼‰
df = handle_missing_values(df)
df = remove_outliers(df)
df = standardize_formats(df)
df = remove_duplicates(df)

# 3. æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆ1-2å¤©ï¼‰
plot_distributions(df)
analyze_correlations(df)
check_class_balance(df)
identify_issues(df)

# 4. ç‰¹å¾å·¥ç¨‹ï¼ˆ2-3å¤©ï¼‰
df = create_new_features(df)
df = encode_categorical(df)
df = scale_numerical(df)
X_selected = select_features(df)

# 5. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆåŠå¤©ï¼‰
X_balanced, y_balanced = balance_classes(X, y)

# 6. æ‹†åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(...)

# 7. æ¨¡å‹é€‰æ‹©ï¼ˆåŠå¤©ï¼‰
baseline_models = [LogisticRegression(), SVC(), RandomForest(), ...]
results = compare_models(baseline_models, X_train, y_train)

# 8. è°ƒå‚ï¼ˆ1-2å¤©ï¼‰
best_model = tune_hyperparameters(
    selected_model,
    param_grid,
    X_train, y_train
)

# 9. è¯„ä¼°ï¼ˆåŠå¤©ï¼‰
evaluate_comprehensive(best_model, X_test, y_test)
explain_predictions(best_model, X_test)

# 10. éƒ¨ç½²ï¼ˆ1-2å¤©ï¼‰
create_api(best_model)
dockerize()
deploy_to_production()

# 11. ç›‘æ§ï¼ˆæŒç»­ï¼‰
monitor_performance()
detect_data_drift()
trigger_retraining_if_needed()

# æ€»è®¡ï¼š10-15å¤©ï¼Œå‡ åƒè¡Œä»£ç 
```

---

## ğŸ“Š æ—¶é—´åˆ†é…çš„çœŸç›¸

```
å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼ˆ100å°æ—¶ï¼‰ï¼š

ğŸ“¦ æ•°æ®ç›¸å…³å·¥ä½œï¼š        60å°æ—¶ (60%)
   â”œâ”€ æ”¶é›†å’Œæ¸…æ´—: 30å°æ—¶
   â”œâ”€ æ¢ç´¢åˆ†æ: 15å°æ—¶
   â””â”€ ç‰¹å¾å·¥ç¨‹: 15å°æ—¶

ğŸ¤– å»ºæ¨¡ç›¸å…³å·¥ä½œï¼š        20å°æ—¶ (20%)
   â”œâ”€ æ¨¡å‹é€‰æ‹©: 5å°æ—¶
   â”œâ”€ è°ƒå‚: 10å°æ—¶  â† ä½ ä»¥ä¸ºæ˜¯å…¨éƒ¨
   â””â”€ è¯„ä¼°: 5å°æ—¶

ğŸš€ å·¥ç¨‹å’Œéƒ¨ç½²ï¼š          15å°æ—¶ (15%)
   â”œâ”€ APIå¼€å‘: 5å°æ—¶
   â”œâ”€ éƒ¨ç½²: 5å°æ—¶
   â””â”€ ç›‘æ§ç³»ç»Ÿ: 5å°æ—¶

ğŸ“ æ–‡æ¡£å’Œæ±‡æŠ¥ï¼š          5å°æ—¶ (5%)
```

---

## ğŸ’¡ å…³é”®ç»“è®º

### âŒ é”™è¯¯è®¤è¯†
```
"æœºå™¨å­¦ä¹  = è°ƒç”¨sklearn + è°ƒè°ƒå‚æ•°"
```

### âœ… æ­£ç¡®è®¤è¯†
```
æœºå™¨å­¦ä¹  = 
    æ·±å…¥ç†è§£ä¸šåŠ¡é—®é¢˜ +
    æ”¶é›†å’Œæ¸…æ´—æ•°æ®ï¼ˆæœ€è€—æ—¶ï¼‰+
    åˆ›é€ æ€§çš„ç‰¹å¾å·¥ç¨‹ï¼ˆæœ€å…³é”®ï¼‰+
    ç³»ç»ŸåŒ–çš„æ¨¡å‹é€‰æ‹© +
    ç§‘å­¦çš„è°ƒå‚ï¼ˆä¸æ˜¯çè¯•ï¼‰+
    å…¨é¢çš„è¯„ä¼°ï¼ˆä¸åªå‡†ç¡®ç‡ï¼‰+
    å·¥ç¨‹åŒ–éƒ¨ç½² +
    æŒç»­ç›‘æ§å’Œä¼˜åŒ–
```

---

## ğŸ“ ä¸åŒæ°´å¹³çš„åŒºåˆ«

### åˆå­¦è€…æ°´å¹³
```python
# æ‹¿åˆ°å¹²å‡€çš„æ•°æ®
X, y = load_clean_data()

# è°ƒç”¨åº“
from sklearn.svm import SVC
model = SVC()
model.fit(X, y)

# çœ‹å‡†ç¡®ç‡
print(model.score(X_test, y_test))

# å®Œæˆï¼
```

### ä¸­çº§å·¥ç¨‹å¸ˆæ°´å¹³
```python
# è‡ªå·±æ¸…æ´—æ•°æ®
df = load_raw_data()
df = clean_data(df)

# å°è¯•å¤šä¸ªæ¨¡å‹
models = [SVC(), RandomForest(), ...]
best_model = compare_and_select(models)

# è°ƒå‚
grid_search = GridSearchCV(best_model, param_grid)
grid_search.fit(X, y)

# å®Œæ•´è¯„ä¼°
evaluate_model(grid_search.best_estimator_)
```

### é«˜çº§å·¥ç¨‹å¸ˆæ°´å¹³
```python
# 1. ç†è§£ä¸šåŠ¡é—®é¢˜ï¼Œå®šä¹‰æ­£ç¡®çš„ç›®æ ‡
business_metric = define_business_objective()

# 2. ç³»ç»ŸåŒ–çš„æ•°æ®pipeline
data = build_data_pipeline(sources)

# 3. åˆ›é€ æ€§çš„ç‰¹å¾å·¥ç¨‹
features = engineer_features(data, domain_knowledge)

# 4. å¤„ç†æ•°æ®è´¨é‡é—®é¢˜
features = handle_imbalance(features)
features = handle_outliers(features)

# 5. æ¨¡å‹ensemble
model = ensemble_models([model1, model2, model3])

# 6. é’ˆå¯¹ä¸šåŠ¡ä¼˜åŒ–
threshold = optimize_for_business_metric(model, business_metric)

# 7. å¯è§£é‡Šæ€§
explanations = explain_model(model)

# 8. ç”Ÿäº§åŒ–éƒ¨ç½²
api = deploy_with_monitoring(model)

# 9. æŒç»­ä¼˜åŒ–
setup_ab_test(new_model, old_model)
setup_retraining_pipeline()
```

---

## ğŸš€ ç»™ä½ çš„å»ºè®®

### 1. ä¸è¦è½»è§†æ•°æ®å·¥ä½œ
```
"Garbage in, garbage out"

æœ€å¥½çš„æ¨¡å‹ + çƒ‚æ•°æ® = çƒ‚ç»“æœ
æ™®é€šçš„æ¨¡å‹ + å¥½æ•°æ® = å¥½ç»“æœ
```

### 2. ç‰¹å¾å·¥ç¨‹æ˜¯æ ¸å¿ƒç«äº‰åŠ›
```
è°ƒå‚èƒ½æå‡: 2-5%
ç‰¹å¾å·¥ç¨‹èƒ½æå‡: 10-30%

è€Œä¸”ç‰¹å¾å·¥ç¨‹éœ€è¦ï¼š
- é¢†åŸŸçŸ¥è¯†
- åˆ›é€ åŠ›
- ç»éªŒ
â†’ ä¸æ˜¯ç°æˆçš„åº“èƒ½è§£å†³çš„ï¼
```

### 3. ç†è§£ä¸šåŠ¡æ¯”ç†è§£ç®—æ³•æ›´é‡è¦
```
é”™è¯¯çš„ç›®æ ‡ + å®Œç¾çš„æ¨¡å‹ = æ— ç”¨
æ­£ç¡®çš„ç›®æ ‡ + ç®€å•çš„æ¨¡å‹ = æœ‰ä»·å€¼
```

### 4. å­¦ä¹ è·¯çº¿
```
ç¬¬1é˜¶æ®µ: è°ƒç”¨åº“ï¼Œç†è§£åŸºç¡€ç®—æ³•
         â†“
ç¬¬2é˜¶æ®µ: å­¦ä¹ æ•°æ®å¤„ç†ï¼Œç‰¹å¾å·¥ç¨‹
         â†“
ç¬¬3é˜¶æ®µ: ç†è§£ä¸šåŠ¡ï¼Œç«¯åˆ°ç«¯é¡¹ç›®
         â†“
ç¬¬4é˜¶æ®µ: å¤§è§„æ¨¡éƒ¨ç½²ï¼ŒMLOps
```

---

## ğŸ“š æ¨èèµ„æº

### æ•°æ®å¤„ç†
- Pandasæ•™ç¨‹
- SQLç†Ÿç»ƒæŒæ¡
- æ•°æ®å¯è§†åŒ–ï¼ˆMatplotlib, Seabornï¼‰

### ç‰¹å¾å·¥ç¨‹
- ã€ŠFeature Engineering for Machine Learningã€‹
- Kaggleç«èµ›ï¼ˆå­¦ä¹ é«˜æ‰‹çš„ç‰¹å¾å·¥ç¨‹æŠ€å·§ï¼‰

### ç«¯åˆ°ç«¯é¡¹ç›®
- ã€ŠMachine Learning Yearningã€‹ - Andrew Ng
- å®é™…é¡¹ç›®ç»éªŒï¼ˆæœ€é‡è¦ï¼ï¼‰

### MLOps
- Docker, Kubernetes
- CI/CD pipeline
- æ¨¡å‹ç›‘æ§

---

## ğŸ¯ æ€»ç»“

ä½ è¯´çš„**"è°ƒç”¨åº“+è°ƒå‚"**ï¼š
- âœ… å¯¹äº**å­¦ä¹ é˜¶æ®µ**æ¥è¯´ï¼Œè¿™æ ·ç†è§£æ²¡é—®é¢˜
- âœ… å¯¹äº**ç†è§£ç®—æ³•åŸç†**æ¥è¯´ï¼Œå¤Ÿç”¨
- âŒ å¯¹äº**å®é™…é¡¹ç›®**æ¥è¯´ï¼Œè¿™åªæ˜¯å†°å±±ä¸€è§’

**çœŸç›¸**ï¼š
- ğŸ“Š **æ•°æ®å·¥ä½œå 60%** - æœ€è„æœ€ç´¯ä½†æœ€å…³é”®
- ğŸ”§ **ç‰¹å¾å·¥ç¨‹å 20%** - æœ€æœ‰åˆ›é€ åŠ›ï¼Œæœ€èƒ½æå‡æ€§èƒ½
- ğŸ¤– **å»ºæ¨¡è°ƒå‚å 10%** - åº“ç¡®å®å¸®äº†å¤§å¿™ï¼Œä½†ä¹Ÿéœ€è¦ç³»ç»Ÿæ–¹æ³•
- ğŸš€ **å·¥ç¨‹éƒ¨ç½²å 10%** - è®©æ¨¡å‹çœŸæ­£äº§ç”Ÿä»·å€¼

**å¥½æ¶ˆæ¯**ï¼š
ç°æˆçš„åº“ï¼ˆsklearn, TensorFlowï¼‰ç¡®å®è®©å»ºæ¨¡å˜ç®€å•äº†ï¼
è¿™è®©ä½ æœ‰æ›´å¤šæ—¶é—´ä¸“æ³¨äºï¼š
- ç†è§£ä¸šåŠ¡
- æ¸…æ´—æ•°æ®
- åˆ›é€ ç‰¹å¾
- è§£å†³å®é™…é—®é¢˜

è¿™äº›æ‰æ˜¯æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆçš„æ ¸å¿ƒä»·å€¼ï¼ğŸ’

