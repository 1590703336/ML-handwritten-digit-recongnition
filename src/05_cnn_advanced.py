"""
ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œ(CNN)è¯†åˆ«æ‰‹å†™æ•°å­—
CNNæ˜¯å›¾åƒè¯†åˆ«çš„æœ€ä½³ç®—æ³•ï¼
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'Microsoft YaHei', 'STHeiti', 'Heiti TC']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†æ·±åº¦å­¦ä¹ åº“
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    HAS_TF = True
    print(f"âœ… TensorFlowç‰ˆæœ¬: {tf.__version__}")
except ImportError:
    HAS_TF = False
    print("âŒ æœªå®‰è£…TensorFlowï¼Œæ­£åœ¨ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ...")

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import TensorDataset, DataLoader
    HAS_TORCH = True
    print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
except ImportError:
    HAS_TORCH = False
    print("âŒ æœªå®‰è£…PyTorch")

print("="*80)
print("ğŸš€ å·ç§¯ç¥ç»ç½‘ç»œ (CNN) - å›¾åƒè¯†åˆ«çš„ç‹è€…")
print("="*80)

# åŠ è½½æ•°æ®
digits = datasets.load_digits()
X = digits.images  # ä¿æŒ8x8å½¢çŠ¶
y = digits.target

print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
print(f"   æ€»æ ·æœ¬æ•°: {len(X)}")
print(f"   å›¾ç‰‡å°ºå¯¸: 8x8")
print(f"   ç±»åˆ«æ•°: 10")

# æ‹†åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"   è®­ç»ƒé›†: {len(X_train)}")
print(f"   æµ‹è¯•é›†: {len(X_test)}")

# ============ CNNæ–¹æ¡ˆ1: TensorFlow/Keras ============
if HAS_TF:
    print("\n" + "="*80)
    print("ã€æ–¹æ¡ˆ1ã€‘ä½¿ç”¨ TensorFlow/Keras å®ç°CNN")
    print("="*80)
    
    # æ•°æ®é¢„å¤„ç†
    X_train_tf = X_train.reshape(-1, 8, 8, 1).astype('float32') / 16.0  # å½’ä¸€åŒ–
    X_test_tf = X_test.reshape(-1, 8, 8, 1).astype('float32') / 16.0
    y_train_tf = keras.utils.to_categorical(y_train, 10)  # one-hotç¼–ç 
    y_test_tf = keras.utils.to_categorical(y_test, 10)
    
    # æ„å»ºCNNæ¨¡å‹
    print("\nğŸ—ï¸  æ„å»ºCNNæ¨¡å‹...")
    model_keras = keras.Sequential([
        # ç¬¬ä¸€å±‚å·ç§¯ï¼šæå–åŸºç¡€ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€è§’ç‚¹ï¼‰
        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', 
                     input_shape=(8, 8, 1), padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        
        # ç¬¬äºŒå±‚å·ç§¯ï¼šæå–æ›´å¤æ‚çš„ç‰¹å¾ï¼ˆå½¢çŠ¶ã€çº¹ç†ï¼‰
        layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        
        # å±•å¹³
        layers.Flatten(),
        
        # å…¨è¿æ¥å±‚
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),  # é˜²æ­¢è¿‡æ‹Ÿåˆ
        
        # è¾“å‡ºå±‚
        layers.Dense(10, activation='softmax')
    ])
    
    # ç¼–è¯‘æ¨¡å‹
    model_keras.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # æ˜¾ç¤ºæ¨¡å‹ç»“æ„
    print("\nğŸ“ æ¨¡å‹ç»“æ„:")
    model_keras.summary()
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸ§  å¼€å§‹è®­ç»ƒCNN...")
    history = model_keras.fit(
        X_train_tf, y_train_tf,
        batch_size=32,
        epochs=50,
        verbose=1,
        validation_data=(X_test_tf, y_test_tf)
    )
    
    # è¯„ä¼°æ¨¡å‹
    test_loss, test_acc = model_keras.evaluate(X_test_tf, y_test_tf, verbose=0)
    print(f"\nâœ… CNN (TensorFlow) æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc*100:.2f}%")
    
    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å‡†ç¡®ç‡æ›²çº¿
    axes[0].plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
    axes[0].plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('å‡†ç¡®ç‡', fontsize=12)
    axes[0].set_title('CNNè®­ç»ƒè¿‡ç¨‹ - å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=11)
    axes[0].grid(True, alpha=0.3)
    
    # æŸå¤±æ›²çº¿
    axes[1].plot(history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
    axes[1].plot(history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('æŸå¤±', fontsize=12)
    axes[1].set_title('CNNè®­ç»ƒè¿‡ç¨‹ - æŸå¤±', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=11)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('cnn_training_history.png', dpi=150)
    print("âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º: cnn_training_history.png")
    plt.show()
    
    # é¢„æµ‹ç¤ºä¾‹
    predictions = model_keras.predict(X_test_tf[:20], verbose=0)
    pred_labels = np.argmax(predictions, axis=1)
    
    # å¯è§†åŒ–é¢„æµ‹ç»“æœ
    fig, axes = plt.subplots(4, 5, figsize=(12, 10))
    fig.suptitle('CNNé¢„æµ‹ç»“æœç¤ºä¾‹', fontsize=16, fontweight='bold')
    
    for i, ax in enumerate(axes.flat):
        ax.imshow(X_test[i], cmap='gray')
        pred_label = pred_labels[i]
        true_label = y_test[i]
        confidence = predictions[i][pred_label] * 100
        
        title = f'çœŸå®: {true_label}\né¢„æµ‹: {pred_label}\nç½®ä¿¡åº¦: {confidence:.1f}%'
        color = 'green' if pred_label == true_label else 'red'
        ax.set_title(title, fontsize=9, color=color, fontweight='bold')
        ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('cnn_predictions.png', dpi=150)
    print("âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜ä¸º: cnn_predictions.png")
    plt.show()
    
    # å¯è§†åŒ–å·ç§¯å±‚å­¦åˆ°çš„ç‰¹å¾
    print("\nğŸ” å¯è§†åŒ–CNNå­¦åˆ°çš„ç‰¹å¾...")
    
    # æå–ç¬¬ä¸€å±‚å·ç§¯å±‚çš„æƒé‡
    first_conv_layer = model_keras.layers[0]
    filters, biases = first_conv_layer.get_weights()
    
    # æ˜¾ç¤ºå‰16ä¸ªå·ç§¯æ ¸
    fig, axes = plt.subplots(4, 8, figsize=(12, 6))
    fig.suptitle('CNNç¬¬ä¸€å±‚å·ç§¯æ ¸ï¼ˆç‰¹å¾æå–å™¨ï¼‰', fontsize=14, fontweight='bold')
    
    for i, ax in enumerate(axes.flat):
        if i < min(32, filters.shape[3]):
            ax.imshow(filters[:, :, 0, i], cmap='viridis')
            ax.set_title(f'Filter {i+1}', fontsize=8)
        ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('cnn_filters.png', dpi=150)
    print("âœ… å·ç§¯æ ¸å¯è§†åŒ–å·²ä¿å­˜ä¸º: cnn_filters.png")
    plt.show()
    
    # ä¿å­˜æ¨¡å‹
    model_keras.save('digit_cnn_model.h5')
    print("\nğŸ’¾ CNNæ¨¡å‹å·²ä¿å­˜ä¸º: digit_cnn_model.h5")

# ============ CNNæ–¹æ¡ˆ2: PyTorch ============
elif HAS_TORCH:
    print("\n" + "="*80)
    print("ã€æ–¹æ¡ˆ2ã€‘ä½¿ç”¨ PyTorch å®ç°CNN")
    print("="*80)
    
    # æ•°æ®é¢„å¤„ç†
    X_train_pt = torch.FloatTensor(X_train).unsqueeze(1) / 16.0  # æ·»åŠ é€šé“ç»´åº¦
    X_test_pt = torch.FloatTensor(X_test).unsqueeze(1) / 16.0
    y_train_pt = torch.LongTensor(y_train)
    y_test_pt = torch.LongTensor(y_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_pt, y_train_pt)
    test_dataset = TensorDataset(X_test_pt, y_test_pt)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # å®šä¹‰CNNæ¨¡å‹
    class DigitCNN(nn.Module):
        def __init__(self):
            super(DigitCNN, self).__init__()
            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
            self.pool = nn.MaxPool2d(2, 2)
            self.fc1 = nn.Linear(64 * 2 * 2, 128)
            self.fc2 = nn.Linear(128, 10)
            self.dropout = nn.Dropout(0.5)
        
        def forward(self, x):
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = x.view(-1, 64 * 2 * 2)
            x = F.relu(self.fc1(x))
            x = self.dropout(x)
            x = self.fc2(x)
            return x
    
    model_torch = DigitCNN()
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model_torch.parameters(), lr=0.001)
    
    print("\nğŸ§  å¼€å§‹è®­ç»ƒCNN (PyTorch)...")
    
    train_losses = []
    test_accuracies = []
    
    for epoch in range(50):
        model_torch.train()
        epoch_loss = 0
        for images, labels in train_loader:
            optimizer.zero_grad()
            outputs = model_torch(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        
        # è¯„ä¼°
        model_torch.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in test_loader:
                outputs = model_torch(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        acc = 100 * correct / total
        train_losses.append(epoch_loss / len(train_loader))
        test_accuracies.append(acc)
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/50], Loss: {epoch_loss/len(train_loader):.4f}, Accuracy: {acc:.2f}%')
    
    print(f"\nâœ… CNN (PyTorch) æœ€ç»ˆå‡†ç¡®ç‡: {test_accuracies[-1]:.2f}%")
    
    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    fig, ax = plt.subplots(1, 1, figsize=(10, 5))
    ax.plot(test_accuracies, linewidth=2)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
    ax.set_title('CNN (PyTorch) è®­ç»ƒè¿‡ç¨‹', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('cnn_pytorch_training.png', dpi=150)
    plt.show()
    
    # ä¿å­˜æ¨¡å‹
    torch.save(model_torch.state_dict(), 'digit_cnn_pytorch.pth')
    print("\nğŸ’¾ CNNæ¨¡å‹å·²ä¿å­˜ä¸º: digit_cnn_pytorch.pth")

# ============ å¦‚æœæ²¡æœ‰æ·±åº¦å­¦ä¹ åº“ ============
else:
    print("\n" + "="*80)
    print("âš ï¸  æœªæ£€æµ‹åˆ°æ·±åº¦å­¦ä¹ åº“ (TensorFlow æˆ– PyTorch)")
    print("="*80)
    print("""
è¦ä½¿ç”¨CNNï¼Œè¯·å®‰è£…ä»¥ä¸‹å…¶ä¸­ä¸€ä¸ªåº“ï¼š

æ–¹æ³•1: å®‰è£…TensorFlow
    pip install tensorflow

æ–¹æ³•2: å®‰è£…PyTorch (æ¨èç”¨äºç ”ç©¶)
    # CPUç‰ˆæœ¬
    pip install torch torchvision
    
    # GPUç‰ˆæœ¬ (CUDAæ”¯æŒ)
    è¯·è®¿é—®: https://pytorch.org/get-started/locally/

å®‰è£…åé‡æ–°è¿è¡Œæ­¤è„šæœ¬å³å¯ï¼
""")
    
    # ä½¿ç”¨scikit-learnçš„MLPä½œä¸ºæ›¿ä»£æ¼”ç¤º
    print("ä½œä¸ºæ¼”ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœº(MLP)æ¨¡æ‹Ÿ...")
    from sklearn.neural_network import MLPClassifier
    from sklearn import metrics
    import time
    
    X_train_flat = X_train.reshape(len(X_train), -1) / 16.0
    X_test_flat = X_test.reshape(len(X_test), -1) / 16.0
    
    mlp = MLPClassifier(
        hidden_layer_sizes=(128, 64),
        max_iter=100,
        random_state=42,
        verbose=True
    )
    
    print("\nè®­ç»ƒMLPç¥ç»ç½‘ç»œ...")
    mlp.fit(X_train_flat, y_train)
    
    pred = mlp.predict(X_test_flat)
    acc = metrics.accuracy_score(y_test, pred)
    
    print(f"\nâœ… MLPå‡†ç¡®ç‡: {acc*100:.2f}%")
    print("\nğŸ’¡ æç¤º: å®‰è£…TensorFlowå¯ä»¥è·å¾—æ›´å¥½çš„CNNå®ç°å’Œæ›´é«˜çš„å‡†ç¡®ç‡ï¼")

# ============ CNNåŸç†è§£é‡Š ============
print("\n" + "="*80)
print("ğŸ“š CNNåŸç†ç®€ä»‹")
print("="*80)
print("""
CNN (å·ç§¯ç¥ç»ç½‘ç»œ) ä¸ºä»€ä¹ˆæœ€é€‚åˆå›¾åƒè¯†åˆ«ï¼Ÿ

1ï¸âƒ£  å·ç§¯å±‚ (Convolutional Layer)
   - è‡ªåŠ¨å­¦ä¹ ç‰¹å¾æå–å™¨ï¼ˆä¸éœ€è¦æ‰‹å·¥è®¾è®¡ï¼‰
   - ç¬¬ä¸€å±‚ï¼šå­¦ä¹ è¾¹ç¼˜ã€è§’ç‚¹
   - ç¬¬äºŒå±‚ï¼šå­¦ä¹ å½¢çŠ¶ã€çº¹ç†
   - ç¬¬ä¸‰å±‚ï¼šå­¦ä¹ æ›´å¤æ‚çš„æ¨¡å¼
   
   ç¤ºä¾‹ï¼šè¯†åˆ«æ•°å­—"7"
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ åŸå§‹å›¾ç‰‡ â”‚ --> â”‚ è¾¹ç¼˜æ£€æµ‹ â”‚ --> â”‚ å½¢çŠ¶è¯†åˆ« â”‚ --> "è¿™æ˜¯7"
   â”‚  (8x8)  â”‚     â”‚(3x3å·ç§¯)â”‚     â”‚(3x3å·ç§¯)â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2ï¸âƒ£  æ± åŒ–å±‚ (Pooling Layer)
   - é™ä½ç»´åº¦ï¼Œå‡å°‘è®¡ç®—é‡
   - æå–æœ€é‡è¦çš„ç‰¹å¾
   - å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ï¼ˆå¯¹å¹³ç§»ã€æ—‹è½¬ä¸æ•æ„Ÿï¼‰

3ï¸âƒ£  å…¨è¿æ¥å±‚ (Fully Connected Layer)
   - å°†æå–çš„ç‰¹å¾ç»„åˆèµ·æ¥
   - æœ€ç»ˆåˆ†ç±»å†³ç­–

å¯¹æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼š

KNN:    ç›´æ¥æ¯”è¾ƒåƒç´  â†’ ç®€å•ä½†æ•ˆæœå·®
SVM:    æ‰‹å·¥è®¾è®¡ç‰¹å¾ â†’ éœ€è¦ä¸“ä¸šçŸ¥è¯†
CNN:    è‡ªåŠ¨å­¦ä¹ ç‰¹å¾ â†’ æ•ˆæœæœ€å¥½ï¼â­

CNNçš„ä¼˜åŠ¿ï¼š
âœ… è‡ªåŠ¨ç‰¹å¾å­¦ä¹ ï¼ˆä¸éœ€è¦äººå·¥è®¾è®¡ï¼‰
âœ… å±€éƒ¨è¿æ¥ï¼ˆå…³æ³¨å±€éƒ¨ç‰¹å¾ï¼‰
âœ… å‚æ•°å…±äº«ï¼ˆåŒä¸€ä¸ªç‰¹å¾æ£€æµ‹å™¨ç”¨äºæ•´å¼ å›¾ï¼‰
âœ… å¹³ç§»ä¸å˜æ€§ï¼ˆæ•°å­—åœ¨å“ªé‡Œéƒ½èƒ½è¯†åˆ«ï¼‰

åœ¨å¤§æ•°æ®é›†ï¼ˆå¦‚MNIST 28x28, ImageNetï¼‰ä¸Šï¼š
CNN >> ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
""")

print("\n" + "="*80)
print("ğŸ¯ æ€»ç»“")
print("="*80)
print("""
æ‰‹å†™æ•°å­—è¯†åˆ« - å„ç®—æ³•é€‚ç”¨åœºæ™¯ï¼š

ğŸ“š KNN:
   - ç”¨äº: æ•™å­¦ã€ç†è§£æœºå™¨å­¦ä¹ åŸºç¡€
   - ä¼˜ç‚¹: ç®€å•æ˜“æ‡‚
   - ç¼ºç‚¹: æ•ˆæœå·®ã€é¢„æµ‹æ…¢
   - å‡†ç¡®ç‡: ~90-95%

ğŸ¯ SVM/éšæœºæ£®æ—:
   - ç”¨äº: ä¸­å°å‹æ•°æ®é›†ã€ç‰¹å¾æ˜ç¡®çš„ä»»åŠ¡
   - ä¼˜ç‚¹: è®­ç»ƒå¿«ã€æ•ˆæœå¥½
   - ç¼ºç‚¹: éœ€è¦ç‰¹å¾å·¥ç¨‹
   - å‡†ç¡®ç‡: ~96-98%

ğŸ§  MLPç¥ç»ç½‘ç»œ:
   - ç”¨äº: é€šç”¨æœºå™¨å­¦ä¹ ä»»åŠ¡
   - ä¼˜ç‚¹: è‡ªåŠ¨å­¦ä¹ ç‰¹å¾
   - ç¼ºç‚¹: ä¸å¦‚CNNé€‚åˆå›¾åƒ
   - å‡†ç¡®ç‡: ~97-98%

ğŸš€ CNN:
   - ç”¨äº: å›¾åƒè¯†åˆ«ï¼ˆæœ€ä½³é€‰æ‹©ï¼ï¼‰
   - ä¼˜ç‚¹: è‡ªåŠ¨ç‰¹å¾å­¦ä¹ ã€å‡†ç¡®ç‡æœ€é«˜
   - ç¼ºç‚¹: éœ€è¦è¾ƒå¤šæ•°æ®ã€è®­ç»ƒè¾ƒæ…¢
   - å‡†ç¡®ç‡: ~99%+

æ¨èè·¯çº¿ï¼š
1. å­¦ä¹ : ä»KNNå¼€å§‹ç†è§£åŸºç¡€æ¦‚å¿µ
2. å®è·µ: ç”¨SVM/MLPå¤„ç†å®é™…é—®é¢˜
3. è¿›é˜¶: ç”¨CNNå¤„ç†å›¾åƒä»»åŠ¡
4. é«˜çº§: å­¦ä¹ ResNetã€Transformerç­‰å…ˆè¿›æ¶æ„
""")
print("="*80)

