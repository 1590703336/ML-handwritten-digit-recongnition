import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, metrics
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import time

print("="*70)
print("ğŸš€ æå‡KNNæ¨¡å‹ç²¾åº¦çš„å¤šç§æ–¹æ³•")
print("="*70)

# åŠ è½½æ•°æ®
digits = datasets.load_digits()
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

# ============ æ–¹æ³•1: åŸºçº¿æ¨¡å‹ï¼ˆåŸå§‹æ•°æ®ï¼ŒK=3ï¼‰ ============
print("\nã€æ–¹æ³•1ã€‘åŸºçº¿æ¨¡å‹ï¼šåŸå§‹æ•°æ® + K=3")
X_train, X_test, y_train, y_test = train_test_split(
    data, digits.target, test_size=0.5, shuffle=False
)

classifier1 = KNeighborsClassifier(n_neighbors=3)
classifier1.fit(X_train, y_train)
pred1 = classifier1.predict(X_test)
acc1 = metrics.accuracy_score(y_test, pred1)
print(f"  å‡†ç¡®ç‡: {acc1:.4f} ({acc1*100:.2f}%)")

# ============ æ–¹æ³•2: ä¼˜åŒ–Kå€¼ ============
print("\nã€æ–¹æ³•2ã€‘ä¼˜åŒ–Kå€¼ï¼šå¯»æ‰¾æœ€ä½³K")
best_k = 1
best_acc = 0
for k in range(1, 31):
    clf = KNeighborsClassifier(n_neighbors=k)
    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    acc = metrics.accuracy_score(y_test, pred)
    if acc > best_acc:
        best_acc = acc
        best_k = k

classifier2 = KNeighborsClassifier(n_neighbors=best_k)
classifier2.fit(X_train, y_train)
pred2 = classifier2.predict(X_test)
acc2 = metrics.accuracy_score(y_test, pred2)
print(f"  æœ€ä½³Kå€¼: {best_k}")
print(f"  å‡†ç¡®ç‡: {acc2:.4f} ({acc2*100:.2f}%) â†‘ æå‡: {(acc2-acc1)*100:.2f}%")

# ============ æ–¹æ³•3: æ•°æ®æ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰ ============
print("\nã€æ–¹æ³•3ã€‘æ•°æ®æ ‡å‡†åŒ–ï¼šè®©æ¯ä¸ªç‰¹å¾çš„æƒé‡ç›¸åŒ")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

classifier3 = KNeighborsClassifier(n_neighbors=3)
classifier3.fit(X_train_scaled, y_train)
pred3 = classifier3.predict(X_test_scaled)
acc3 = metrics.accuracy_score(y_test, pred3)
print(f"  å‡†ç¡®ç‡: {acc3:.4f} ({acc3*100:.2f}%) â†‘ æå‡: {(acc3-acc1)*100:.2f}%")

# ============ æ–¹æ³•4: æ”¹å˜è·ç¦»åº¦é‡æ–¹å¼ ============
print("\nã€æ–¹æ³•4ã€‘æ”¹å˜è·ç¦»åº¦é‡ï¼šæµ‹è¯•ä¸åŒçš„è·ç¦»è®¡ç®—æ–¹æ³•")
# æ¬§æ°è·ç¦» (é»˜è®¤)
clf_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')
clf_euclidean.fit(X_train, y_train)
pred_euc = clf_euclidean.predict(X_test)
acc_euc = metrics.accuracy_score(y_test, pred_euc)
print(f"  æ¬§æ°è·ç¦»: {acc_euc:.4f} ({acc_euc*100:.2f}%)")

# æ›¼å“ˆé¡¿è·ç¦»
clf_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')
clf_manhattan.fit(X_train, y_train)
pred_man = clf_manhattan.predict(X_test)
acc_man = metrics.accuracy_score(y_test, pred_man)
print(f"  æ›¼å“ˆé¡¿è·ç¦»: {acc_man:.4f} ({acc_man*100:.2f}%)")

# é—µå¯å¤«æ–¯åŸºè·ç¦»
clf_minkowski = KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=3)
clf_minkowski.fit(X_train, y_train)
pred_min = clf_minkowski.predict(X_test)
acc_min = metrics.accuracy_score(y_test, pred_min)
print(f"  é—µå¯å¤«æ–¯åŸºè·ç¦»: {acc_min:.4f} ({acc_min*100:.2f}%)")

best_metric_acc = max(acc_euc, acc_man, acc_min)
print(f"  æœ€ä½³è·ç¦»åº¦é‡æå‡: {(best_metric_acc-acc1)*100:.2f}%")

# ============ æ–¹æ³•5: å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆæ›´å¤§çš„è®­ç»ƒé›†ï¼‰ ============
print("\nã€æ–¹æ³•5ã€‘å¢åŠ è®­ç»ƒæ•°æ®ï¼šä»50%å¢åŠ åˆ°80%")
X_train_large, X_test_small, y_train_large, y_test_small = train_test_split(
    data, digits.target, test_size=0.2, shuffle=False
)

classifier5 = KNeighborsClassifier(n_neighbors=3)
classifier5.fit(X_train_large, y_train_large)
pred5 = classifier5.predict(X_test_small)
acc5 = metrics.accuracy_score(y_test_small, pred5)
print(f"  è®­ç»ƒé›†å¤§å°: {len(X_train_large)} å¼ ï¼ˆåŸæ¥æ˜¯ {len(X_train)} å¼ ï¼‰")
print(f"  å‡†ç¡®ç‡: {acc5:.4f} ({acc5*100:.2f}%)")
print(f"  æ³¨æ„ï¼šæµ‹è¯•é›†å˜å°äº†ï¼Œæ‰€ä»¥ä¸èƒ½ç›´æ¥æ¯”è¾ƒ")

# ============ æ–¹æ³•6: åŠ æƒæŠ•ç¥¨ï¼ˆè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰ ============
print("\nã€æ–¹æ³•6ã€‘åŠ æƒæŠ•ç¥¨ï¼šè·ç¦»è¿‘çš„é‚»å±…æƒé‡æ›´å¤§")
classifier6 = KNeighborsClassifier(n_neighbors=5, weights='distance')
classifier6.fit(X_train, y_train)
pred6 = classifier6.predict(X_test)
acc6 = metrics.accuracy_score(y_test, pred6)
print(f"  å‡†ç¡®ç‡: {acc6:.4f} ({acc6*100:.2f}%) â†‘ æå‡: {(acc6-acc1)*100:.2f}%")

# ============ æ–¹æ³•7: ç»„åˆå¤šç§ä¼˜åŒ–ï¼ˆç»ˆæç‰ˆæœ¬ï¼‰ ============
print("\nã€æ–¹æ³•7ã€‘ç»„åˆä¼˜åŒ–ï¼šæ ‡å‡†åŒ– + æœ€ä½³K + åŠ æƒæŠ•ç¥¨ + æœ€ä½³è·ç¦»")
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

classifier7 = KNeighborsClassifier(
    n_neighbors=best_k, 
    weights='distance',
    metric='manhattan'
)
classifier7.fit(X_train_scaled, y_train)
pred7 = classifier7.predict(X_test_scaled)
acc7 = metrics.accuracy_score(y_test, pred7)
print(f"  å‡†ç¡®ç‡: {acc7:.4f} ({acc7*100:.2f}%) â†‘ æå‡: {(acc7-acc1)*100:.2f}%")

# ============ æ€»ç»“å¯¹æ¯” ============
print("\n" + "="*70)
print("ğŸ“Š å„æ–¹æ³•å‡†ç¡®ç‡å¯¹æ¯”æ€»ç»“ï¼š")
print("="*70)

methods = [
    'æ–¹æ³•1: åŸºçº¿æ¨¡å‹ï¼ˆK=3ï¼‰',
    f'æ–¹æ³•2: ä¼˜åŒ–Kå€¼ï¼ˆK={best_k}ï¼‰',
    'æ–¹æ³•3: æ•°æ®æ ‡å‡†åŒ–',
    'æ–¹æ³•6: åŠ æƒæŠ•ç¥¨',
    'æ–¹æ³•7: ç»„åˆä¼˜åŒ–'
]
accuracies = [acc1, acc2, acc3, acc6, acc7]
improvements = [0, (acc2-acc1)*100, (acc3-acc1)*100, (acc6-acc1)*100, (acc7-acc1)*100]

for method, acc, imp in zip(methods, accuracies, improvements):
    if imp > 0:
        print(f"{method:30s} â†’ {acc:.4f} ({acc*100:.2f}%) [+{imp:.2f}%]")
    else:
        print(f"{method:30s} â†’ {acc:.4f} ({acc*100:.2f}%)")

print("\n" + "="*70)
print("ğŸ’¡ æå‡ç²¾åº¦çš„æ–¹æ³•æ€»ç»“ï¼š")
print("="*70)
print("1. âœ… ä¼˜åŒ–Kå€¼ - é€šè¿‡å®éªŒæ‰¾åˆ°æœ€ä½³K")
print("2. âœ… æ•°æ®æ ‡å‡†åŒ– - è®©ä¸åŒç‰¹å¾çš„æƒé‡å¹³è¡¡")
print("3. âœ… æ”¹å˜è·ç¦»åº¦é‡ - å°è¯•ä¸åŒçš„è·ç¦»è®¡ç®—æ–¹æ³•")
print("4. âœ… å¢åŠ è®­ç»ƒæ•°æ® - æ›´å¤šæ•°æ®é€šå¸¸èƒ½æå‡ç²¾åº¦")
print("5. âœ… åŠ æƒæŠ•ç¥¨ - è·ç¦»è¿‘çš„é‚»å±…æƒé‡æ›´å¤§")
print("6. âœ… æ•°æ®å¢å¼º - å¯¹å›¾ç‰‡è¿›è¡Œæ—‹è½¬ã€å¹³ç§»ç­‰æ“ä½œ")
print("7. âœ… ç‰¹å¾å·¥ç¨‹ - æå–æ›´æœ‰æ„ä¹‰çš„ç‰¹å¾")
print("8. âœ… ä½¿ç”¨æ›´å¼ºå¤§çš„ç®—æ³• - å¦‚SVMã€éšæœºæ£®æ—ã€ç¥ç»ç½‘ç»œ")
print("="*70)

# å¯è§†åŒ–å¯¹æ¯”
plt.figure(figsize=(14, 6))
colors = ['gray', 'blue', 'green', 'orange', 'red']
bars = plt.bar(range(len(methods)), [a*100 for a in accuracies], color=colors, alpha=0.7)

# åœ¨æŸ±å­ä¸Šæ ‡æ³¨å‡†ç¡®ç‡
for i, (bar, acc) in enumerate(zip(bars, accuracies)):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{acc*100:.2f}%',
             ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.xlabel('ä¼˜åŒ–æ–¹æ³•', fontsize=13)
plt.ylabel('å‡†ç¡®ç‡ (%)', fontsize=13)
plt.title('ä¸åŒä¼˜åŒ–æ–¹æ³•å¯¹KNNæ¨¡å‹å‡†ç¡®ç‡çš„æå‡æ•ˆæœ', fontsize=15, fontweight='bold')
plt.xticks(range(len(methods)), 
           ['åŸºçº¿\n(K=3)', f'ä¼˜åŒ–K\n(K={best_k})', 'æ ‡å‡†åŒ–', 'åŠ æƒ\næŠ•ç¥¨', 'ç»„åˆ\nä¼˜åŒ–'],
           fontsize=11)
plt.ylim([min(accuracies)*100-2, 100])
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

print("\nğŸ‰ å®éªŒå®Œæˆï¼ç°åœ¨ä½ çŸ¥é“å¦‚ä½•æå‡KNNæ¨¡å‹çš„ç²¾åº¦äº†ï¼")

