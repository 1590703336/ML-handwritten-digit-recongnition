"""
é«˜ç²¾åº¦æ‰‹å†™æ•°å­—è¯†åˆ« - å®Œæ•´MNISTæ•°æ®é›†
ä½¿ç”¨28Ã—28å›¾åƒ + æ‰€æœ‰ä¼˜åŒ–æŠ€å·§
ç›®æ ‡å‡†ç¡®ç‡: 99.5%+
"""

import numpy as np
import matplotlib.pyplot as plt
import warnings
import time
warnings.filterwarnings('ignore')

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'Microsoft YaHei', 'STHeiti', 'Heiti TC']
plt.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, models
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
    print(f"âœ… TensorFlowç‰ˆæœ¬: {tf.__version__}")
    HAS_TF = True
except ImportError:
    print("âŒ æœªå®‰è£…TensorFlowï¼Œè¯·è¿è¡Œ: pip install tensorflow")
    HAS_TF = False
    exit(1)

print("="*80)
print("ğŸš€ é«˜ç²¾åº¦æ‰‹å†™æ•°å­—è¯†åˆ« - MNISTå®Œæ•´æ•°æ®é›† (28Ã—28)")
print("="*80)

# ============ 1. åŠ è½½MNISTæ•°æ®é›† ============
print("\nğŸ“Š åŠ è½½MNISTæ•°æ®é›†...")
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
print(f"   è®­ç»ƒé›†: {len(X_train):,} å¼ å›¾ç‰‡")
print(f"   æµ‹è¯•é›†: {len(X_test):,} å¼ å›¾ç‰‡")
print(f"   å›¾ç‰‡å°ºå¯¸: {X_train.shape[1]}Ã—{X_train.shape[2]}")
print(f"   ç±»åˆ«æ•°: 10 (æ•°å­—0-9)")

# æ•°æ®é¢„å¤„ç†
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

print(f"\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆ:")
print(f"   è¾“å…¥å½¢çŠ¶: {X_train.shape}")
print(f"   æ ‡ç­¾å½¢çŠ¶: {y_train_cat.shape}")
print(f"   æ•°å€¼èŒƒå›´: {X_train.min():.1f} ~ {X_train.max():.1f}")

# å¯è§†åŒ–ä¸€äº›æ ·æœ¬
fig, axes = plt.subplots(2, 10, figsize=(15, 3))
fig.suptitle('MNISTæ•°æ®é›†æ ·æœ¬ï¼ˆ28Ã—28é«˜åˆ†è¾¨ç‡ï¼‰', fontsize=14, fontweight='bold')
for i in range(20):
    ax = axes[i//10, i%10]
    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')
    ax.set_title(f'{y_train[i]}', fontsize=12, fontweight='bold')
    ax.axis('off')
plt.tight_layout()
plt.savefig('outputs/visualizations/08_mnist_samples.png', dpi=150, bbox_inches='tight')
print("\nâœ… æ ·æœ¬å¯è§†åŒ–å·²ä¿å­˜: outputs/visualizations/08_mnist_samples.png")
plt.show()

# ============ 2. æ„å»ºæ”¹è¿›çš„CNNæ¨¡å‹ ============
print("\n" + "="*80)
print("ğŸ—ï¸  æ„å»ºé«˜ç²¾åº¦CNNæ¨¡å‹")
print("="*80)

def build_improved_cnn():
    """æ„å»ºæ”¹è¿›çš„CNNæ¨¡å‹ï¼ŒåŒ…å«å¤šé¡¹ä¼˜åŒ–æŠ€å·§"""
    model = keras.Sequential([
        # è¾“å…¥å±‚
        layers.Input(shape=(28, 28, 1)),
        
        # ç¬¬ä¸€ç»„å·ç§¯å—ï¼ˆ32ä¸ªæ»¤æ³¢å™¨ï¼‰
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),  # æ‰¹å½’ä¸€åŒ–ï¼šåŠ é€Ÿè®­ç»ƒã€æé«˜ç¨³å®šæ€§
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),  # Dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
        
        # ç¬¬äºŒç»„å·ç§¯å—ï¼ˆ64ä¸ªæ»¤æ³¢å™¨ï¼‰
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # ç¬¬ä¸‰ç»„å·ç§¯å—ï¼ˆ128ä¸ªæ»¤æ³¢å™¨ï¼‰
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.4),
        
        # å…¨è¿æ¥å±‚
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        
        # è¾“å‡ºå±‚
        layers.Dense(10, activation='softmax')
    ])
    
    return model

model = build_improved_cnn()

# ç¼–è¯‘æ¨¡å‹ - ä½¿ç”¨Adamä¼˜åŒ–å™¨
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\nğŸ“ æ¨¡å‹æ¶æ„:")
model.summary()

# è®¡ç®—æ¨¡å‹å‚æ•°
total_params = model.count_params()
print(f"\nğŸ“Š æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")

# ============ 3. æ•°æ®å¢å¼º ============
print("\n" + "="*80)
print("ğŸ¨ é…ç½®æ•°æ®å¢å¼º")
print("="*80)

datagen = ImageDataGenerator(
    rotation_range=10,        # éšæœºæ—‹è½¬Â±10åº¦
    width_shift_range=0.1,    # æ°´å¹³å¹³ç§»Â±10%
    height_shift_range=0.1,   # å‚ç›´å¹³ç§»Â±10%
    zoom_range=0.1,           # éšæœºç¼©æ”¾Â±10%
    shear_range=0.1,          # å‰ªåˆ‡å˜æ¢
)

print("âœ… æ•°æ®å¢å¼ºé…ç½®:")
print("   - éšæœºæ—‹è½¬: Â±10Â°")
print("   - éšæœºå¹³ç§»: Â±10%")
print("   - éšæœºç¼©æ”¾: Â±10%")
print("   - å‰ªåˆ‡å˜æ¢: Â±10%")

# å¯è§†åŒ–æ•°æ®å¢å¼ºæ•ˆæœ
print("\nğŸ” å¯è§†åŒ–æ•°æ®å¢å¼ºæ•ˆæœ...")
sample_img = X_train[0].reshape(1, 28, 28, 1)
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
fig.suptitle('æ•°æ®å¢å¼ºæ•ˆæœå±•ç¤º', fontsize=14, fontweight='bold')

axes[0, 0].imshow(sample_img[0, :, :, 0], cmap='gray')
axes[0, 0].set_title('åŸå§‹å›¾åƒ', fontsize=10, fontweight='bold')
axes[0, 0].axis('off')

augmented_imgs = datagen.flow(sample_img, batch_size=1)
for i in range(1, 10):
    ax = axes[i//5, i%5]
    aug_img = next(augmented_imgs)[0]
    ax.imshow(aug_img[:, :, 0], cmap='gray')
    ax.set_title(f'å¢å¼º {i}', fontsize=10)
    ax.axis('off')

plt.tight_layout()
plt.savefig('outputs/visualizations/09_data_augmentation.png', dpi=150, bbox_inches='tight')
print("âœ… æ•°æ®å¢å¼ºå¯è§†åŒ–å·²ä¿å­˜: outputs/visualizations/09_data_augmentation.png")
plt.show()

# ============ 4. é…ç½®è®­ç»ƒå›è°ƒå‡½æ•° ============
print("\n" + "="*80)
print("âš™ï¸  é…ç½®è®­ç»ƒä¼˜åŒ–ç­–ç•¥")
print("="*80)

# æ—©åœï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,              # 15ä¸ªepochæ²¡æœ‰æ”¹å–„å°±åœæ­¢
    restore_best_weights=True,
    verbose=1
)

# å­¦ä¹ ç‡è¡°å‡ï¼šåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,               # å­¦ä¹ ç‡å‡åŠ
    patience=5,               # 5ä¸ªepochæ²¡æœ‰æ”¹å–„å°±é™ä½å­¦ä¹ ç‡
    min_lr=1e-7,
    verbose=1
)

# æ¨¡å‹æ£€æŸ¥ç‚¹ï¼šä¿å­˜æœ€ä½³æ¨¡å‹
checkpoint = ModelCheckpoint(
    'models/mnist_cnn_best.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

print("âœ… è®­ç»ƒç­–ç•¥é…ç½®:")
print("   - æ—©åœæœºåˆ¶: 15ä¸ªepochæ— æ”¹å–„åˆ™åœæ­¢")
print("   - å­¦ä¹ ç‡è¡°å‡: 5ä¸ªepochæ— æ”¹å–„åˆ™é™ä½50%")
print("   - æ¨¡å‹æ£€æŸ¥ç‚¹: è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹")

# ============ 5. è®­ç»ƒæ¨¡å‹ ============
print("\n" + "="*80)
print("ğŸ§  å¼€å§‹è®­ç»ƒé«˜ç²¾åº¦CNNæ¨¡å‹")
print("="*80)
print("\nâ° è®­ç»ƒå¼€å§‹æ—¶é—´:", time.strftime("%Y-%m-%d %H:%M:%S"))
print("ğŸ’¡ æç¤º: ä½¿ç”¨æ•°æ®å¢å¼ºè®­ç»ƒï¼Œé¢„è®¡éœ€è¦5-10åˆ†é’Ÿ...\n")

start_time = time.time()

# ä½¿ç”¨æ•°æ®å¢å¼ºè¿›è¡Œè®­ç»ƒ
history = model.fit(
    datagen.flow(X_train, y_train_cat, batch_size=128),
    epochs=100,  # æœ€å¤šè®­ç»ƒ100ä¸ªepochï¼Œæ—©åœä¼šè‡ªåŠ¨åœæ­¢
    steps_per_epoch=len(X_train) // 128,
    validation_data=(X_test, y_test_cat),
    callbacks=[early_stopping, lr_scheduler, checkpoint],
    verbose=1
)

training_time = time.time() - start_time

print("\n" + "="*80)
print("âœ… è®­ç»ƒå®Œæˆï¼")
print("="*80)
print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
print(f"ğŸ“ˆ å®é™…è®­ç»ƒè½®æ•°: {len(history.history['loss'])} epochs")

# ============ 6. è¯„ä¼°æ¨¡å‹ ============
print("\n" + "="*80)
print("ğŸ“Š æ¨¡å‹è¯„ä¼°")
print("="*80)

# åŠ è½½æœ€ä½³æ¨¡å‹
best_model = keras.models.load_model('models/mnist_cnn_best.h5')

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
test_loss, test_acc = best_model.evaluate(X_test, y_test_cat, verbose=0)
print(f"\nâœ… æœ€ç»ˆæµ‹è¯•é›†æ€§èƒ½:")
print(f"   å‡†ç¡®ç‡: {test_acc*100:.4f}%")
print(f"   æŸå¤±: {test_loss:.4f}")

# è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
from sklearn.metrics import classification_report, confusion_matrix

y_pred = best_model.predict(X_test, verbose=0)
y_pred_classes = np.argmax(y_pred, axis=1)

print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_classes, target_names=[str(i) for i in range(10)]))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred_classes)
fig, ax = plt.subplots(figsize=(10, 8))

# ä½¿ç”¨matplotlibç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆä¸ä¾èµ–seabornï¼‰
im = ax.imshow(cm, cmap='Blues', interpolation='nearest')
ax.figure.colorbar(im, ax=ax)

# è®¾ç½®åˆ»åº¦
ax.set_xticks(np.arange(10))
ax.set_yticks(np.arange(10))
ax.set_xticklabels(range(10))
ax.set_yticklabels(range(10))

# åœ¨æ¯ä¸ªæ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼
for i in range(10):
    for j in range(10):
        text = ax.text(j, i, cm[i, j],
                      ha="center", va="center",
                      color="white" if cm[i, j] > cm.max()/2 else "black",
                      fontsize=10, fontweight='bold')

ax.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
ax.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
ax.set_title('æ··æ·†çŸ©é˜µ - é«˜ç²¾åº¦CNN', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('outputs/visualizations/10_confusion_matrix.png', dpi=150, bbox_inches='tight')
print("\nâœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: outputs/visualizations/10_confusion_matrix.png")
plt.show()

# ============ 7. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ ============
print("\n" + "="*80)
print("ğŸ“ˆ å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹")
print("="*80)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å‡†ç¡®ç‡æ›²çº¿
axes[0].plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
axes[0].plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('å‡†ç¡®ç‡', fontsize=12)
axes[0].set_title('è®­ç»ƒè¿‡ç¨‹ - å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)
max_acc = max(history.history['val_accuracy'])
axes[0].axhline(y=max_acc, color='r', linestyle='--', alpha=0.5, 
                label=f'æœ€ä½³: {max_acc*100:.2f}%')
axes[0].legend(fontsize=11)

# æŸå¤±æ›²çº¿
axes[1].plot(history.history['loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
axes[1].plot(history.history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('æŸå¤±', fontsize=12)
axes[1].set_title('è®­ç»ƒè¿‡ç¨‹ - æŸå¤±', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('outputs/visualizations/11_training_history.png', dpi=150, bbox_inches='tight')
print("âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: outputs/visualizations/11_training_history.png")
plt.show()

# ============ 8. é¢„æµ‹å±•ç¤º ============
print("\n" + "="*80)
print("ğŸ¯ é¢„æµ‹ç»“æœå±•ç¤º")
print("="*80)

# éšæœºé€‰æ‹©30ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹
np.random.seed(42)
sample_indices = np.random.choice(len(X_test), 30, replace=False)
sample_images = X_test[sample_indices]
sample_labels = y_test[sample_indices]

predictions = best_model.predict(sample_images, verbose=0)
pred_labels = np.argmax(predictions, axis=1)

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
fig, axes = plt.subplots(6, 5, figsize=(15, 18))
fig.suptitle('é«˜ç²¾åº¦CNNé¢„æµ‹ç»“æœï¼ˆ28Ã—28 MNISTï¼‰', fontsize=16, fontweight='bold')

for i, ax in enumerate(axes.flat):
    ax.imshow(sample_images[i].reshape(28, 28), cmap='gray')
    pred_label = pred_labels[i]
    true_label = sample_labels[i]
    confidence = predictions[i][pred_label] * 100
    
    title = f'çœŸå®: {true_label} | é¢„æµ‹: {pred_label}\nç½®ä¿¡åº¦: {confidence:.1f}%'
    color = 'green' if pred_label == true_label else 'red'
    ax.set_title(title, fontsize=9, color=color, fontweight='bold')
    ax.axis('off')

plt.tight_layout()
plt.savefig('outputs/visualizations/12_predictions.png', dpi=150, bbox_inches='tight')
print("âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: outputs/visualizations/12_predictions.png")
plt.show()

# ç»Ÿè®¡é”™è¯¯é¢„æµ‹
errors = np.where(pred_labels != sample_labels)[0]
print(f"\nğŸ“Š åœ¨30ä¸ªæ ·æœ¬ä¸­:")
print(f"   æ­£ç¡®é¢„æµ‹: {30-len(errors)} ä¸ª")
print(f"   é”™è¯¯é¢„æµ‹: {len(errors)} ä¸ª")
if len(errors) > 0:
    print(f"   é”™è¯¯ç‡: {len(errors)/30*100:.1f}%")

# ============ 9. å¯è§†åŒ–å­¦ä¹ åˆ°çš„ç‰¹å¾ ============
print("\n" + "="*80)
print("ğŸ”¬ å¯è§†åŒ–CNNå­¦ä¹ åˆ°çš„ç‰¹å¾")
print("="*80)

# æå–ç¬¬ä¸€å±‚å·ç§¯å±‚çš„æƒé‡
first_conv_layer = best_model.layers[0]
filters, biases = first_conv_layer.get_weights()

print(f"\nå·ç§¯æ ¸ä¿¡æ¯:")
print(f"   å½¢çŠ¶: {filters.shape}")
print(f"   æ•°é‡: {filters.shape[3]} ä¸ª")

# æ˜¾ç¤ºå‰32ä¸ªå·ç§¯æ ¸
fig, axes = plt.subplots(4, 8, figsize=(16, 8))
fig.suptitle('ç¬¬ä¸€å±‚å·ç§¯æ ¸ï¼ˆè¾¹ç¼˜å’Œçº¹ç†æ£€æµ‹å™¨ï¼‰', fontsize=14, fontweight='bold')

for i, ax in enumerate(axes.flat):
    if i < filters.shape[3]:
        filter_img = filters[:, :, 0, i]
        ax.imshow(filter_img, cmap='viridis')
        ax.set_title(f'Filter {i+1}', fontsize=9)
    ax.axis('off')

plt.tight_layout()
plt.savefig('outputs/visualizations/13_cnn_filters.png', dpi=150, bbox_inches='tight')
print("âœ… å·ç§¯æ ¸å¯è§†åŒ–å·²ä¿å­˜: outputs/visualizations/13_cnn_filters.png")
plt.show()

# å¯è§†åŒ–ç‰¹å¾å›¾
print("\nğŸ” å¯è§†åŒ–ç‰¹å¾å›¾ï¼ˆFeature Mapsï¼‰...")

# åˆ›å»ºä¸€ä¸ªæ¨¡å‹æ¥è¾“å‡ºä¸­é—´å±‚çš„æ¿€æ´»
layer_outputs = [layer.output for layer in best_model.layers[:7]]  # å‰7å±‚
activation_model = keras.Model(inputs=best_model.input, outputs=layer_outputs)

# é€‰æ‹©ä¸€ä¸ªæ ·æœ¬
sample = X_test[0:1]
activations = activation_model.predict(sample, verbose=0)

# å¯è§†åŒ–ç¬¬ä¸€å±‚å·ç§¯å±‚çš„è¾“å‡º
first_layer_activation = activations[0]
fig, axes = plt.subplots(4, 8, figsize=(16, 8))
fig.suptitle('ç¬¬ä¸€å±‚å·ç§¯å±‚è¾“å‡ºï¼ˆç‰¹å¾å›¾ï¼‰', fontsize=14, fontweight='bold')

for i, ax in enumerate(axes.flat):
    if i < first_layer_activation.shape[3]:
        ax.imshow(first_layer_activation[0, :, :, i], cmap='viridis')
        ax.set_title(f'ç‰¹å¾å›¾ {i+1}', fontsize=9)
    ax.axis('off')

plt.tight_layout()
plt.savefig('outputs/visualizations/14_feature_maps.png', dpi=150, bbox_inches='tight')
print("âœ… ç‰¹å¾å›¾å¯è§†åŒ–å·²ä¿å­˜: outputs/visualizations/14_feature_maps.png")
plt.show()

# ============ 10. æ€§èƒ½å¯¹æ¯”æ€»ç»“ ============
print("\n" + "="*80)
print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
print("="*80)

print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   æ¨¡å‹æ€§èƒ½å¯¹æ¯”                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ¨¡å‹          â”‚  æ•°æ®é›†   â”‚  å‡†ç¡®ç‡   â”‚      è¯´æ˜        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ KNN             â”‚ 8Ã—8       â”‚ ~95%      â”‚ ç®€å•ä½†æ•ˆæœä¸€èˆ¬   â”‚
â”‚ SVM/RF          â”‚ 8Ã—8       â”‚ ~98%      â”‚ ä¼ ç»ŸMLæœ€ä½³       â”‚
â”‚ åŸºç¡€CNN         â”‚ 8Ã—8       â”‚ ~98%      â”‚ æ·±åº¦å­¦ä¹ å…¥é—¨     â”‚
â”‚ ğŸ† é«˜ç²¾åº¦CNN    â”‚ 28Ã—28     â”‚ ~99.5%+   â”‚ ä¸“ä¸šçº§è§£å†³æ–¹æ¡ˆ   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

print(f"\nâœ¨ æœ¬æ¬¡è®­ç»ƒæœ€ç»ˆç»“æœ:")
print(f"   ğŸ“ˆ æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc*100:.4f}%")
print(f"   â±ï¸  è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
print(f"   ğŸ’¾ æ¨¡å‹å¤§å°: {os.path.getsize('models/mnist_cnn_best.h5')/1024/1024:.2f} MB")

print("\nğŸ¯ åº”ç”¨çš„ä¼˜åŒ–æŠ€å·§:")
print("   âœ… ä½¿ç”¨å®Œæ•´MNISTæ•°æ®é›†ï¼ˆ60,000è®­ç»ƒæ ·æœ¬ï¼‰")
print("   âœ… æ›´æ·±çš„CNNæ¶æ„ï¼ˆ3ç»„å·ç§¯å—ï¼‰")
print("   âœ… æ‰¹å½’ä¸€åŒ–ï¼ˆBatchNormalizationï¼‰")
print("   âœ… Dropoutæ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰")
print("   âœ… æ•°æ®å¢å¼ºï¼ˆæ—‹è½¬ã€å¹³ç§»ã€ç¼©æ”¾ï¼‰")
print("   âœ… å­¦ä¹ ç‡è¡°å‡ç­–ç•¥")
print("   âœ… æ—©åœæœºåˆ¶")
print("   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")

print("\n" + "="*80)
print("ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: models/mnist_cnn_best.h5")
print("="*80)

# ä¿å­˜è®­ç»ƒå†å²
import pickle
with open('outputs/reports/training_history.pkl', 'wb') as f:
    pickle.dump(history.history, f)
print("\nâœ… è®­ç»ƒå†å²å·²ä¿å­˜: outputs/reports/training_history.pkl")

print("\nğŸ’¡ å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹:")
print("""
from tensorflow import keras
import numpy as np

# åŠ è½½æ¨¡å‹
model = keras.models.load_model('models/mnist_cnn_best.h5')

# é¢„æµ‹
image = your_image.reshape(1, 28, 28, 1) / 255.0  # å½’ä¸€åŒ–
prediction = model.predict(image)
digit = np.argmax(prediction)
print(f'é¢„æµ‹æ•°å­—: {digit}')
""")

import os

